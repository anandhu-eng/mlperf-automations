alias: get-ml-model-llama3
automation_alias: script
automation_uid: 5b4e0237da074764
cache: true
category: AI/ML models
env:
  CM_ML_MODEL_DATASET: ''
input_mapping:
  checkpoint: LLAMA3_CHECKPOINT_PATH
  path: CM_LLAMA3_MODEL_DOWNLOAD_PATH
new_env_keys:
- CM_ML_MODEL_*
- LLAMA3_CHECKPOINT_PATH
prehook_deps:
- enable_if_env:
    CM_TMP_REQUIRE_DOWNLOAD:
    - 'yes'
  env: {}
  extra_cache_tags: llama3,llama-3
  force_env_keys:
  - CM_GIT_CHECKOUT_FOLDER
  names:
  - hf-zoo
  tags: get,ml-model,huggingface,zoo,_clone-repo
print_env_at_the_end:
  LLAMA3_CHECKPOINT_PATH: LLAMA3 checkpoint path
tags:
- get
- raw
- ml-model
- language-processing
- llama3
- llama3-405b
uid: 2f8cef2acc334e80
variations:
  fp16:
    default: true
    env:
      CM_ML_MODEL_INPUT_DATA_TYPES: fp16
      CM_ML_MODEL_PRECISION: fp16
      CM_ML_MODEL_WEIGHT_DATA_TYPES: fp16
    group: precision
  meta-llama/Llama-3.1-405B-Instruct:
    adr:
      hf-zoo:
        tags: _model-stub.meta-llama/Llama-3.1-405B-Instruct
    default: true
    env:
      CM_GIT_CHECKOUT_FOLDER: <<<CM_LLAMA3_MODEL_DOWNLOAD_PATH>>>/Llama-3-405b-instruct
      CM_MODEL_ZOO_ENV_KEY: LLAMA3
    group: huggingface-stub
  meta-llama/Llama-3.1-8B-Instruct:
    adr:
      hf-zoo:
        tags: _model-stub.meta-llama/Llama-3.1-8B-Instruct
    env:
      CM_GIT_CHECKOUT_FOLDER: <<<CM_LLAMA3_MODEL_DOWNLOAD_PATH>>>/Llama-3-8b-instruct
      CM_MODEL_ZOO_ENV_KEY: LLAMA3
    group: huggingface-stub
  vllm:
    default: true
    env:
      CM_ML_MODEL_FRAMEWORK: vllm
    group: framework
  stub.#:
    adr:
      hf-zoo:
        tags: _model-stub.#
    env:
      CM_MODEL_ZOO_ENV_KEY: LLAMA3
    group: huggingface-stub
docker:
  use_host_group_id: True
  use_host_user_id: True
  real_run: False
  pass_user_group: True #useful if docker is run by a different user fromt he one who built it and under the same group
  pre_run_cmds:
    #- cm pull repo && cm run script --tags=get,git,repo,_repo.https://github.com/GATEOverflow/inference_results_v4.0.git --update
    - cm pull repo
  mounts:
   - "${{ CM_OUTDIRNAME }}:${{ CM_OUTDIRNAME }}"
   - "${{ CM_LLAMA3_MODEL_DOWNLOAD_PATH }}:${{ CM_LLAMA3_MODEL_DOWNLOAD_PATH }}"
  skip_run_cmd: 'no'
  shm_size: '32gb'
  interactive: True
  extra_run_args: ' --dns 8.8.8.8 --dns 8.8.4.4 --cap-add SYS_ADMIN --cap-add SYS_TIME --security-opt apparmor=unconfined --security-opt seccomp=unconfined'
  os: ubuntu
  cm_repo: mlcommons@mlperf-automations
  cm_repo_branch: llm
  os_version: '22.04'
  docker_input_mapping:
    outdirname: CM_OUTDIRNAME
    path: CM_LLAMA3_MODEL_DOWNLOAD_PATH
